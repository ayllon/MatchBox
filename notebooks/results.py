import os
import zipfile
from typing import Tuple, Any, Mapping, Iterable, Union, List, Dict

import numpy as np
import pandas

notebook_dir = os.path.dirname(__file__)


def read_nind(zip: zipfile.ZipFile, run_id: str, ids: Iterable[str], out: Dict[int, List[str]]):
    for id in ids:
        path = os.path.join(run_id, id[:2], id, 'nind.txt')
        for line in zip.open(path):
            arity, ind = line.decode('utf-8').split(' ', maxsplit=1)
            arity = int(arity)
            if ind[0].isdigit():
                _, ind = ind.split(' ', maxsplit=1)
            if arity not in out:
                out[arity] = list()
            out[arity].append(ind.strip())


def load_results(run_ids: Union[List[str], str],
                 basedir: str = os.path.join(notebook_dir, '..', 'results'),
                 return_inds: bool = False) -> Tuple[pandas.DataFrame, Mapping]:
    """
    Load a set of results from a tar file. It is expected to contain the output
    generated by /bin/benchmark.py

    Parameters
    ----------
    run_ids : str or list of strings
        Run identifiers, which is the name of the zip without the extension
    basedir : str
        Directory where the results are stored
    return_inds : bool
        If True, return also the found IND

    Returns
    -------
    out : pair
        The first one corresponds to the results of Find2, and the second
        is a dictionary where the key corresponds to the parametrization of FindQ
        and the value is the DataFrame with the results.
    """
    if not isinstance(run_ids, list):
        run_ids = [run_ids]

    all_find2 = []
    all_findq = {}
    find2_inds = {}
    findq_inds = {}

    for run_id in run_ids:
        zip = zipfile.ZipFile(os.path.join(basedir, f'{run_id}.zip'))
        try:
            find2 = pandas.read_csv(zip.open(f'{run_id}/find2.csv'))
            find2.fillna(0, inplace=True)
            all_find2.append(find2)
            if return_inds:
                read_nind(zip, run_id, find2['id'], out=find2_inds)
        except KeyError:
            pass
        for m in zip.namelist():
            filename = os.path.basename(m)
            if filename.startswith('findg'):
                name = os.path.splitext(filename)[0]
                parts = name.split('_')
                if len(parts) == 4:
                    _, lambd, gamma, grow = parts
                else:
                    _, lambd, gamma = parts
                    grow = 0
                key = (float(lambd), float(gamma), int(grow))
                if key not in all_findq:
                    all_findq[key] = []
                    findq_inds[key] = {}
                try:
                    findq = pandas.read_csv(zip.open(f'{m}'))
                    findq.fillna(0, inplace=True)
                    all_findq[key].append(findq)
                    if return_inds:
                        read_nind(zip, run_id, all_findq[key][-1]['id'], out=findq_inds[key])
                except Exception as e:
                    raise RuntimeError(f'Failed to parse {m}: {e}')

    if return_inds:
        return pandas.concat(all_find2), {k: pandas.concat(v) for k, v in all_findq.items()}, find2_inds, findq_inds
    else:
        return pandas.concat(all_find2), {k: pandas.concat(v) for k, v in all_findq.items()}


def compute_stats(data: pandas.DataFrame, filter_by: Tuple[str, Any]):
    """
    From a DataFrame containing the summary of the runs of one of the algorithm,
    compute the mean and standard deviation of the run-time and match-ratio

    Notes
    -----
    Since exact matches may have been lost by the unary search, or the bootstrapping, the ratio is computed over the
    possible values to obtain.

    Parameters
    ----------
    data : DataFrame
        As output from benchmark.py
    filter_by: pair of string/value
        Restrict the computation to a subset of the data (i.e. filter only by alpha = 0.1)

    Returns
    -------
    out : first, second and third quartiles for the time and for the match ratio, and number of items
    """
    max_ind_column = None
    for c in data.columns:
        if c.startswith('max_'):
            max_ind_column = c

    timeouts = data[(data[filter_by[0]] == filter_by[1])]['timeout']
    mask = (data[filter_by[0]] == filter_by[1]) & (data['exact'] > 0) & (data['ind'] > 0)
    masked = data[mask]

    if not len(masked):
        return [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, timeouts.sum()]

    match = (masked[max_ind_column] / masked['exact'])
    time_qs = np.quantile(masked['time'], [0.25, 0.75])
    match_qs = np.quantile(match, [0.25, 0.75])
    nind_qs = np.quantile(masked['unique_ind'], [0.25, 0.75])
    precision = (masked['ind'] / masked['tests']).mean()
    overhead = (masked['tests'] / masked['unique_ind']).mean()

    return time_qs.tolist() + match_qs.tolist() + nind_qs.tolist() + [precision, overhead, mask.sum(), timeouts.sum()/len(timeouts)]


def general_stats(find2: pandas.DataFrame, findq: Mapping[Any, pandas.DataFrame], findq_subset: Iterable = None,
                  alpha: float = 0.1):
    """
    Compute the statistics from both find2 and findq results

    Parameters
    ----------
    find2 :
        Find2 results
    findq :
        Set of FindQ results
    findq_subset :
        Use only these keys from the findq dictionary
    alpha :
        Compute statistics for this value of alpha

    Returns
    -------
    out : DataFrame
        A DataFrame with the aggregated information
    """
    if findq_subset is not None:
        findq = dict([(key, findq[key]) for key in findq_subset])

    rows = [['Find2', None, None] + compute_stats(find2, ('bootstrap_alpha', alpha))]

    for (lambd, gamma, grow), v in findq.items():
        rows.append(
            [f'FindQ {grow}', lambd, np.clip(1 - alpha * gamma, 0., 1.)] + compute_stats(v, ('bootstrap_alpha', alpha)))

    return pandas.DataFrame(
        rows, columns=['Method', 'Lambda', 'Gamma', 'Time Q1', 'Time Q3', 'Match Q1', 'Match Q3', 'Card Q1', 'Card Q3',
                       'Precision', 'Overhead', 'N', 'Timeouts']
    ).sort_values(['Method', 'Lambda', 'Gamma'])


def most_frequent_highest(inds: Dict[int, List[str]], topn: int = 1, max_ind: int = None) -> List[str]:
    """
    Return the topn most frequent, highest, arity IND
    """
    if not inds:
        return []
    if max_ind is None:
        max_ind = max(inds.keys())
    highest = inds[max_ind]
    ind, count = np.unique(highest, return_counts=True)
    top = np.flip(count.argsort())
    return list(ind[top[:topn]])


def parse_fields(ind: str) -> Tuple[str, List[str]]:
    """
    Parse a LHS or RHS, into a pair relation name, list of attributes
    """
    dataset, rest = ind.split('::')
    fields = rest.strip()[1:-1].split(', ')
    return dataset.strip(), fields


def pretty_highest_ind(inds: Dict[int, List[str]], topn: int = 1, max_ind: int = None) -> Union[
    List[pandas.DataFrame], pandas.DataFrame]:
    """
    Return a list of (or a single) pandas DataFrame containing the topn most frequent, highest arity, IND found
    """
    mo = most_frequent_highest(inds, topn, max_ind)
    dfs = []
    for m in mo:
        lhs, rhs = m.split('âŠ†')
        lds, lattr = parse_fields(lhs)
        rds, dattr = parse_fields(rhs)
        tab = list(zip(lattr, dattr))
        df = pandas.DataFrame(tab, columns=[lds, rds])
        df.sort_values(by=0, axis='columns', inplace=True)
        dfs.append(df)
    return dfs if topn > 1 else dfs[0]
